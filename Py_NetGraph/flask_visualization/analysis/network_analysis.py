import colorsys
from datetime import datetime, timedelta
from collections import defaultdict
import networkx as nx
import numpy as np
import matplotlib.cm as cm
import matplotlib.colors as mcolors
from community import community_louvain
from database import fetch_data  # ç¡®ä¿æ•°æ®åº“è¿æ¥å’ŒæŸ¥è¯¢å‡½æ•°æ­£ç¡®


def analyze_friends():
    """è·å–å¥½å‹å…³ç³»æ•°æ®å¹¶è®¡ç®—èŠ‚ç‚¹å±æ€§ï¼ŒåŒ…å«æ‰€æœ‰ç”¨æˆ·"""
    # æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·ä¿¡æ¯
    query_users = "SELECT id, username FROM users"
    df_users = fetch_data(query_users)

    # æŸ¥è¯¢å¥½å‹å…³ç³»æ•°æ®
    query_friends = """
        SELECT f.user_id, u1.username AS user_name_1, 
               f.friend_id, u2.username AS user_name_2
        FROM friends f
        JOIN users u1 ON f.user_id = u1.id
        JOIN users u2 ON f.friend_id = u2.id
    """
    df_friends = fetch_data(query_friends)

    # æ„å»ºæ— å‘å›¾
    G = nx.Graph()
    user_map = {row["id"]: row["username"] for _, row in df_users.iterrows()}  # è®°å½•æ‰€æœ‰ç”¨æˆ·

    # æ·»åŠ è¾¹ï¼ˆå¥½å‹å…³ç³»ï¼‰
    for _, row in df_friends.iterrows():
        G.add_edge(row["user_id"], row["friend_id"])

    # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„å¥½å‹æ•°é‡
    degrees = {node: G.degree(node) for node in G.nodes()}

    # ç¡®ä¿æ‰€æœ‰ç”¨æˆ·éƒ½å‚ä¸è®¡ç®—ï¼ˆåŒ…æ‹¬æ²¡æœ‰å¥½å‹çš„ï¼‰
    for user_id in user_map.keys():
        if user_id not in degrees:
            degrees[user_id] = 0  # æ— å¥½å‹

    degree_values = list(degrees.values())
    vmin, vmax = min(degree_values), max(degree_values)

    # ğŸ¨ é¢œè‰²åˆ’åˆ†

    bins = np.linspace(vmin, vmax, 6)

    colors = [cm.Paired(i / 5) for i in range(6)]

    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…é¢œè‰²
    node_colors = [
        mcolors.to_hex(colors[np.digitize(degrees[user_id], bins) - 1])
        for user_id in user_map.keys()
    ]

    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…å¤§å°
    node_sizes = [
        np.interp(degrees[user_id], (vmin, vmax), (600, 5000))
        for user_id in user_map.keys()
    ]

    # æ„é€  JSON æ ¼å¼çš„èŠ‚ç‚¹æ•°æ®
    nodes = [{"id": int(user_id), "username": user_map[user_id], "size": node_sizes[i], "color": node_colors[i],
              "degree": degrees[user_id]}
             for i, user_id in enumerate(user_map.keys())]

    # æ„é€ è¾¹æ•°æ®
    edges = [{"source": int(u), "target": int(v)} for u, v in G.edges()]

    return {"nodes": nodes, "edges": edges}

def analyze_centrality():
    """è®¡ç®—ç”¨æˆ·ä¸­å¿ƒæ€§ï¼ˆæ¥æ”¶æ¶ˆæ¯çš„æ•°é‡ï¼‰"""
    query_users = "SELECT id, username FROM users"
    df_users = fetch_data(query_users)

    query_messages = """
        SELECT m.sender_id, u1.username AS sender_name,
               m.receiver_id, u2.username AS receiver_name,
               COUNT(*) AS weight
        FROM messages m
        JOIN users u1 ON m.sender_id = u1.id
        JOIN users u2 ON m.receiver_id = u2.id
        WHERE m.timestamp >= DATE_SUB(NOW(), INTERVAL 1 MONTH)  -- ä»…æŸ¥è¯¢æœ€è¿‘ 1 ä¸ªæœˆçš„æ•°æ®
        GROUP BY m.sender_id, m.receiver_id
    """

    df_messages = fetch_data(query_messages)

    # æ„å»ºæœ‰å‘å›¾
    G = nx.DiGraph()
    user_map = {row["id"]: row["username"] for _, row in df_users.iterrows()}

    # æ·»åŠ è¾¹
    for _, row in df_messages.iterrows():
        G.add_edge(row["sender_id"], row["receiver_id"], weight=row["weight"])

    # è®¡ç®—ä¸­å¿ƒæ€§ï¼ˆæ¥æ”¶æ¶ˆæ¯çš„æ•°é‡ï¼‰
    node_centrality = {node: sum(d["weight"] for _, _, d in G.in_edges(node, data=True)) for node in G.nodes()}

    # ç¡®ä¿æ‰€æœ‰ç”¨æˆ·éƒ½åŒ…å«ï¼Œå³ä½¿æ²¡æœ‰æ¥æ”¶æ¶ˆæ¯
    for user_id in user_map.keys():
        if user_id not in node_centrality:
            node_centrality[user_id] = 0

    centrality_values = list(node_centrality.values())
    vmin, vmax = min(centrality_values), max(centrality_values)

    bins = np.linspace(vmin, vmax, 6)
    colors = [cm.viridis(i / 5) for i in range(6)]

    node_colors = [
        mcolors.to_hex(colors[np.digitize(node_centrality[user_id], bins) - 1])
        for user_id in user_map.keys()
    ]

    node_sizes = [
        np.interp(node_centrality[user_id], (vmin, vmax), (1000, 6000))
        for user_id in user_map.keys()
    ]

    nodes = [{
        "id": int(user_id),
        "username": user_map[user_id],
        "size": node_sizes[i],
        "color": node_colors[i],
        "centrality": node_centrality[user_id]
    } for i, user_id in enumerate(user_map.keys())]

    edges = [{
        "source": int(u),
        "target": int(v),
        "weight": G[u][v]["weight"]
    } for u, v in G.edges()]

    return {"nodes": nodes, "edges": edges}

def analyze_messages():
    """è·å–æ¶ˆæ¯äº’åŠ¨æ•°æ®å¹¶è®¡ç®—èŠ‚ç‚¹å±æ€§ï¼ŒåŒ…å«æ‰€æœ‰ç”¨æˆ·"""
    # æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·ä¿¡æ¯
    query_users = "SELECT id, username FROM users"
    df_users = fetch_data(query_users)

    # -- æŸ¥è¯¢è¿‘ä¸€ä¸ªæœˆçš„æ¶ˆæ¯äº’åŠ¨æ•°æ®
    query_messages = """
        SELECT m.sender_id, u1.username AS sender_name,
               m.receiver_id, u2.username AS receiver_name,
               COUNT(*) as weight
        FROM messages m
        JOIN users u1 ON m.sender_id = u1.id
        JOIN users u2 ON m.receiver_id = u2.id
        WHERE m.timestamp >= DATE_SUB(NOW(), INTERVAL 1 MONTH)
        GROUP BY m.sender_id, m.receiver_id
    """

    df_messages = fetch_data(query_messages)

    # æ„å»ºæœ‰å‘å›¾
    G = nx.DiGraph()
    user_map = {row["id"]: row["username"] for _, row in df_users.iterrows()}

    # æ·»åŠ è¾¹ï¼ˆæ¶ˆæ¯å‘é€å’Œæ¥æ”¶ï¼‰
    for _, row in df_messages.iterrows():
        G.add_edge(row["sender_id"], row["receiver_id"], weight=row["weight"])

    # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„æ´»è·ƒåº¦ï¼ˆå…¥åº¦+å‡ºåº¦ï¼‰
    node_activity = {node: sum(d["weight"] for _, _, d in G.edges(node, data=True)) for node in G.nodes()}

    # ç¡®ä¿æ‰€æœ‰ç”¨æˆ·éƒ½åŒ…å«ï¼Œå³ä½¿æ²¡æœ‰å‘é€æˆ–æ¥æ”¶æ¶ˆæ¯
    for user_id in user_map.keys():
        if user_id not in node_activity:
            node_activity[user_id] = 0  # æ²¡æœ‰å‘é€/æ¥æ”¶æ¶ˆæ¯

    activity_values = list(node_activity.values())
    vmin, vmax = min(activity_values), max(activity_values)

    # ğŸ¨ é¢œè‰²åˆ’åˆ†
    bins = np.linspace(vmin, vmax, 6)
    colors = [cm.viridis(i / 5) for i in range(6)]

    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…é¢œè‰²
    node_colors = [
        mcolors.to_hex(colors[np.digitize(node_activity[user_id], bins) - 1])
        for user_id in user_map.keys()
    ]

    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…å¤§å°
    node_sizes = [
        np.interp(node_activity[user_id], (vmin, vmax), (1000, 6000))
        for user_id in user_map.keys()
    ]

    # æ„é€  JSON æ ¼å¼çš„èŠ‚ç‚¹æ•°æ®
    nodes = [{
        "id": int(user_id),
        "username": user_map[user_id],
        "size": node_sizes[i],
        "color": node_colors[i],
        "activity": node_activity[user_id]
    } for i, user_id in enumerate(user_map.keys())]

    # æ„é€ è¾¹æ•°æ®
    edges = [{
        "source": int(u),
        "target": int(v),
        "weight": G[u][v]["weight"]
    } for u, v in G.edges()]

    return {"nodes": nodes, "edges": edges}


def analyze_friend_distribution():
    """è®¡ç®—å¥½å‹æ•°é‡çš„åˆ†å¸ƒï¼Œå¹¶è¿”å› JSON ç»“æœ"""
    # è·å–å¥½å‹å…³ç³»æ•°æ®
    query = """
        SELECT f.user_id, u.username AS user_name
        FROM friends f
        JOIN users u ON f.user_id = u.id
    """
    df_friends = fetch_data(query)

    # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„å¥½å‹æ•°é‡
    friend_counts = df_friends["user_id"].value_counts()

    # è®¡ç®—å‡å€¼å’Œä¸­ä½æ•°
    mean_friends = friend_counts.mean()
    median_friends = friend_counts.median()

    # é¢œè‰²æ˜ å°„
    bins = np.linspace(friend_counts.min(), friend_counts.max(), 6)
    colors = [cm.Blues(i / 5) for i in range(6)]  # 6 çº§é¢œè‰²
    color_map = {count: mcolors.to_hex(colors[np.digitize(count, bins) - 1]) for count in friend_counts}

    # æ„é€  JSON æ ¼å¼çš„å¥½å‹æ•°æ®
    friend_data = [
        {"user_id": int(user), "friend_count": int(count), "color": color_map[count]}
        for user, count in friend_counts.items()
    ]

    return {
        "friend_data": friend_data,
        "stats": {"mean_friends": mean_friends, "median_friends": median_friends}
    }


def analyze_messages_pagerank():
    """ä½¿ç”¨ PageRank ç®—æ³•è®¡ç®—ç”¨æˆ·ä¼ æ’­æ€§ï¼ˆä»…é‡‡é›†æœ€è¿‘ä¸€ä¸ªæœˆçš„æ•°æ®ï¼‰"""

    # è®¡ç®—æœ€è¿‘ä¸€ä¸ªæœˆçš„æ—¶é—´èŒƒå›´
    one_month_ago = datetime.now() - timedelta(days=30)
    one_month_ago_str = one_month_ago.strftime('%Y-%m-%d %H:%M:%S')

    # æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·ä¿¡æ¯
    query_users = "SELECT id, username FROM users"
    df_users = fetch_data(query_users)

    # æŸ¥è¯¢æœ€è¿‘ä¸€ä¸ªæœˆçš„æ¶ˆæ¯äº’åŠ¨æ•°æ®
    query_messages = f"""
        SELECT m.sender_id, u1.username AS sender_name,
               m.receiver_id, u2.username AS receiver_name,
               COUNT(*) as weight
        FROM messages m
        JOIN users u1 ON m.sender_id = u1.id
        JOIN users u2 ON m.receiver_id = u2.id
        WHERE m.timestamp >= '{one_month_ago_str}'
        GROUP BY m.sender_id, m.receiver_id
    """
    df_messages = fetch_data(query_messages)

    # æ„å»ºæœ‰å‘å›¾
    G = nx.DiGraph()
    user_map = {row["id"]: row["username"] for _, row in df_users.iterrows()}

    # æ·»åŠ å¸¦æƒé‡çš„è¾¹ï¼ˆæ¶ˆæ¯ä¼ æ’­ï¼‰
    for _, row in df_messages.iterrows():
        G.add_edge(row["sender_id"], row["receiver_id"], weight=row["weight"])

    # ç¡®ä¿æ‰€æœ‰ç”¨æˆ·éƒ½ä½œä¸ºèŠ‚ç‚¹
    G.add_nodes_from(df_users['id'])

    # è®¡ç®— PageRank å€¼
    pagerank_scores = nx.pagerank(G, weight='weight')

    # å½’ä¸€åŒ– PageRank ç»“æœ
    scores_values = np.array(list(pagerank_scores.values()))
    vmin, vmax = scores_values.min(), scores_values.max() if len(scores_values) > 0 else (0, 1)

    # é¢œè‰²åˆ’åˆ†
    bins = np.linspace(vmin, vmax, 6)
    colors = [cm.viridis(i / 5) for i in range(6)]

    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…é¢œè‰²å’Œå¤§å°
    node_colors = [
        mcolors.to_hex(colors[np.digitize(pagerank_scores.get(user_id, 0), bins) - 1])
        for user_id in user_map.keys()
    ]
    node_sizes = [
        np.interp(pagerank_scores.get(user_id, 0), (vmin, vmax), (1000, 6000))
        for user_id in user_map.keys()
    ]

    # æ„é€  JSON æ ¼å¼çš„èŠ‚ç‚¹æ•°æ®
    nodes = [{
        "id": int(user_id),
        "username": user_map[user_id],
        "size": node_sizes[i],
        "color": node_colors[i],
        "pagerank": pagerank_scores.get(user_id, 0)
    } for i, user_id in enumerate(user_map.keys())]

    # æ„é€ è¾¹æ•°æ®
    edges = [{
        "source": int(u),
        "target": int(v),
        "weight": G[u][v]["weight"]
    } for u, v in G.edges()]

    return {"nodes": nodes, "edges": edges}

def analyze_messages_hits(days=30):
    """æŒ‰ç¤¾äº¤ç¤¾åŒºç‹¬ç«‹è®¡ç®— HITS ç®—æ³•çš„ hub å’Œ authority å€¼"""

    # è®¡ç®—æ—¶é—´èŒƒå›´
    one_month_ago = datetime.now() - timedelta(days=days)
    one_month_ago_str = one_month_ago.strftime('%Y-%m-%d %H:%M:%S')

    # è·å–æ‰€æœ‰ç”¨æˆ·
    query_users = "SELECT id, username FROM users"
    df_users = fetch_data(query_users)

    # è·å–æœ€è¿‘ days å¤©çš„æ¶ˆæ¯æ•°æ®
    query_messages = f"""
        SELECT m.sender_id, u1.username AS sender_name,
               m.receiver_id, u2.username AS receiver_name,
               COUNT(*) as weight
        FROM messages m
        JOIN users u1 ON m.sender_id = u1.id
        JOIN users u2 ON m.receiver_id = u2.id
        WHERE m.timestamp >= '{one_month_ago_str}'
        GROUP BY m.sender_id, m.receiver_id
    """
    df_messages = fetch_data(query_messages)

    if df_messages.empty:
        print("âš ï¸ df_messages ä¸ºç©ºï¼Œæ²¡æœ‰äº¤äº’æ•°æ®")
        return {}, {}, {}, df_messages

    # **åˆ›å»ºæœ‰å‘å›¾**
    G = nx.DiGraph()

    # **æ„å»ºç”¨æˆ·æ˜ å°„è¡¨**
    user_map = {row["id"]: row["username"] for _, row in df_users.iterrows()}

    if not user_map:
        print("âš ï¸ user_map ä¸ºç©ºï¼Œå¯èƒ½ users è¡¨æ— æ•°æ®")
        return {}, {}, {}, df_messages

    # **æ·»åŠ è¾¹ï¼ˆæ¶ˆæ¯äº¤äº’ï¼‰**
    for _, row in df_messages.iterrows():
        G.add_edge(int(row["sender_id"]), int(row["receiver_id"]), weight=float(row["weight"]))

    if not G.edges:
        print("âš ï¸ G.edges ä¸ºç©ºï¼ŒHITS è®¡ç®—æ— æ³•è¿›è¡Œ")
        return {}, {}, user_map, df_messages

    print(f"ğŸ“Š å›¾åŒ…å« {G.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {G.number_of_edges()} æ¡è¾¹")

    # **æ£€æµ‹ç¤¾åŒºï¼ˆè¿é€šå­å›¾ï¼‰**
    communities = list(nx.weakly_connected_components(G))
    print(f"ğŸ” æ£€æµ‹åˆ° {len(communities)} ä¸ªç‹¬ç«‹ç¤¾äº¤ç¤¾åŒº")

    # **ç‹¬ç«‹è®¡ç®— HITS**
    hub_scores = {}
    authority_scores = {}

    for i, community in enumerate(communities):
        subG = G.subgraph(community).copy()
        print(f"ğŸ“Œ è®¡ç®—ç¤¾åŒº {i+1}, åŒ…å« {subG.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {subG.number_of_edges()} æ¡è¾¹")

        try:
            hits_scores = nx.hits(subG, max_iter=1000, tol=1e-15, normalized=True)
            sub_hub_scores, sub_authority_scores = hits_scores
        except nx.PowerIterationFailedConvergence:
            print(f"âš ï¸ ç¤¾åŒº {i+1} HITS è®¡ç®—æœªæ”¶æ•›ï¼Œæ”¹ç”¨å…¥åº¦/å‡ºåº¦è®¡ç®—")
            sub_hub_scores = {node: subG.out_degree(node, weight="weight") for node in subG.nodes()}
            sub_authority_scores = {node: subG.in_degree(node, weight="weight") for node in subG.nodes()}

        # åˆå¹¶åˆ°æ€»ç»“æœ
        hub_scores.update(sub_hub_scores)
        authority_scores.update(sub_authority_scores)

    return hub_scores, authority_scores, user_map, df_messages



def get_messages_hits_data(days=30):
    """è¿”å›åŒ…å« hub å’Œ authority çš„æ•°æ®"""
    hub_scores, authority_scores, user_map, df_messages = analyze_messages_hits(days)

    nodes = [
        {
            "id": int(user_id),
            "username": user_map[user_id],
            "authority": round(authority_scores.get(user_id, 0), 6),
            "hub": round(hub_scores.get(user_id, 0), 6),
            "size": int((authority_scores.get(user_id, 0) + hub_scores.get(user_id, 0)) * 1000)
        }
        for user_id in user_map.keys()
    ]

    edges = [
        {"source": int(row["sender_id"]), "target": int(row["receiver_id"]), "weight": float(row["weight"])}
        for _, row in df_messages.iterrows()
    ]

    return {"nodes": nodes, "edges": edges}

def analyze_community():
    """è·å–ç”¨æˆ·ç¤¾åŒºåˆ’åˆ†æ•°æ®"""
    # æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·ä¿¡æ¯
    query_users = "SELECT id, username FROM users"
    df_users = fetch_data(query_users)

    # æŸ¥è¯¢å¥½å‹å…³ç³»æ•°æ®
    query_friends = """
        SELECT f.user_id, u1.username AS user_name_1, 
               f.friend_id, u2.username AS user_name_2
        FROM friends f
        JOIN users u1 ON f.user_id = u1.id
        JOIN users u2 ON f.friend_id = u2.id
    """
    df_friends = fetch_data(query_friends)

    # æ„å»ºæ— å‘å›¾
    G = nx.Graph()
    user_map = {row["id"]: row["username"] for _, row in df_users.iterrows()}  # è®°å½•æ‰€æœ‰ç”¨æˆ·

    # æ·»åŠ è¾¹ï¼ˆå¥½å‹å…³ç³»ï¼‰
    for _, row in df_friends.iterrows():
        G.add_edge(row["user_id"], row["friend_id"])

    # ä½¿ç”¨ Louvain ç®—æ³•è¿›è¡Œç¤¾åŒºåˆ’åˆ†
    partition = community_louvain.best_partition(G)

    # ä¸ºæ¯ä¸ªç¤¾åŒºåˆ†é…é¢œè‰²å’Œç”¨æˆ·
    community_map = defaultdict(list)
    for user_id, community_id in partition.items():
        community_map[community_id].append(user_id)

    # å¤„ç†æ²¡æœ‰åˆ†é…åˆ°ç¤¾åŒºçš„ç”¨æˆ·
    default_community = len(community_map)  # è®¾ç½®ä¸€ä¸ªé»˜è®¤ç¤¾åŒºID
    for user_id in user_map.keys():
        if user_id not in partition:
            partition[user_id] = default_community  # å°†æœªåˆ†é…ç¤¾åŒºçš„ç”¨æˆ·åˆ†é…åˆ°é»˜è®¤ç¤¾åŒº
            community_map[default_community].append(user_id)

    # ä¸ºæ¯ä¸ªç¤¾åŒºåˆ†é…é¢œè‰²
    colors = [
        '#' + ''.join([hex(int(c * 255))[2:].zfill(2) for c in colorsys.hsv_to_rgb(i / len(community_map), 1, 1)])
        for i in range(len(community_map))
    ]
    # æ„é€ èŠ‚ç‚¹æ•°æ®å’Œè¾¹æ•°æ®
    nodes = [
        {"id": int(user_id), "username": user_map.get(user_id, "æœªçŸ¥ç”¨æˆ·"),
         "community": partition.get(user_id), "color": colors[partition[user_id]]}
        for user_id in user_map.keys()
    ]
    edges = [{"source": int(u), "target": int(v)} for u, v in G.edges()]

    return {"nodes": nodes, "edges": edges, "community_map": community_map, "colors": colors}



